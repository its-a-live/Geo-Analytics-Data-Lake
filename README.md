# SocialGeoAnalytics

## Обзор
Проект реализует пайплайн для анализа поведения пользователей в социальной сети на основе данных геолокации. Он обрабатывает данные событий (сообщения, подписки, реакции, регистрации) для создания трёх витрин данных:
1. **Аналитика на уровне пользователей**: Отслеживает актуальный и домашний города пользователей, историю путешествий и местное время событий.
2. **Аналитика на уровне геозон**: Агрегирует количество событий (сообщений, реакций, подписок, регистраций) по городам за неделю и месяц.
3. **Рекомендации друзей**: Определяет пары пользователей, которые подписаны на один канал, ранее не переписывались и находятся на расстоянии не более 1 км друг от друга.

Пайплайн построен с использованием **PySpark** для обработки данных и **Airflow** для автоматизации. Данные хранятся в Data Lake на базе Hadoop. Для оптимизации ресурсов во время разработки и тестирования используется выборка данных.

## Структура Data Lake
Data Lake организован в HDFS для эффективной обработки и аналитики. Структура:

- **/source/data/geo/events**: Сырые данные событий в формате Parquet, содержащие события пользователей (сообщения, подписки, реакции, регистрации) с широтой и долготой для сообщений.
- **/storage/data/city_dict**: Справочник городов (`geo.csv`) в формате CSV, содержащий идентификаторы городов, названия, координаты (широта, долгота) и часовые пояса.
- **/storage/data/sample**: Временное хранилище для выборки данных, используемых при обработке (например, `/storage/data/sample/mart_1`, `mart_2`, `mart_3`).
- **/storage/data/analytics**: Директория для итоговых витрин данных:
  - `/storage/data/analytics/mart_1`: Витрина на уровне пользователей (user_id, act_city, home_city, travel_count, travel_array, local_time).
  - `/storage/data/analytics/mart_2`: Витрина на уровне зон (month, week, zone_id, количество событий за неделю/месяц).
  - `/storage/data/analytics/mart_3`: Витрина рекомендаций друзей (user_left, user_right, processed_dttm, zone_id, local_time).
- **Частота обновления данных**: Витрины обновляются ежедневно через Airflow, обрабатывая последние данные событий.
- **Формат данных**:
  - Сырые события: Parquet для эффективного хранения и запросов.
  - Справочник городов: CSV с разделителем «;».
  - Итоговые витрины: Parquet для совместимости со Spark и инструментами аналитики.

## Структура проекта
- **dags/**: Содержит определение DAG Airflow (`datalake_project_dag.py`) для оркестрации пайплайна.
- **scripts/**: Содержит скрипты PySpark для создания витрин:
  - `step_2_mart.py`: Создаёт витрину аналитики на уровне пользователей.
  - `step_3_mart.py`: Создаёт витрину аналитики на уровне зон.
  - `step_4_mart.py`: Создаёт витрину рекомендаций друзей.
  - `geo.py`: Утилиты для геолокационных вычислений (загрузка справочника городов, выборка событий, расчёт расстояний).

## Детали пайплайна
### Шаг 1: Настройка Data Lake
- Сырые данные событий хранятся в `/source/data/geo/events` (Parquet).
- Справочник городов (`geo.csv`) загружается в `/storage/data/city_dict` с помощью `hdfs dfs -copyFromLocal`.
- Временные выборки данных сохраняются в `/storage/data/sample` для оптимизации обработки при разработке.
- Итоговые витрины записываются в `/storage/data/analytics` в формате Parquet для последующей аналитики.

### Шаг 2: Витрина аналитики на уровне пользователей
- **Скрипт**: `step_2_mart.py`
- **Назначение**: Сопоставляет события с ближайшим городом, определяет актуальный город (место последнего сообщения), домашний город (город, где пользователь находился ≥27 дней), историю путешествий и местное время события.

### Шаг 3: Витрина аналитики на уровне зон
- **Скрипт**: `step_3_mart.py`
- **Назначение**: Агрегирует количество событий (сообщений, реакций, подписок, регистраций) по городам за неделю и месяц для геоаналитики.

### Шаг 4: Витрина рекомендаций друзей
- **Скрипт**: `step_4_mart.py`
- **Назначение**: Определяет пары пользователей, которые подписаны на один канал, ранее не переписывались и находятся на расстоянии ≤1 км друг от друга, для рекомендаций друзей.

### Шаг 5: Автоматизация с помощью Airflow
- **Файл DAG**: `datalake_project_dag.py`
- **Назначение**: Автоматизирует ежедневное обновление трёх витрин данных.

## Инструкции по настройке
1. **Загрузка справочника городов**:
   ```bash
   hdfs dfs -copyFromLocal geo.csv /user/sineie/data/city_dict